{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91376352-bb34-4518-b66c-33ece11f9980",
   "metadata": {},
   "source": [
    "Это улучшенная версия кода, которая гораздо быстрее и стабильнее обучается за счёт:\n",
    "\n",
    "1. NumPy вместо вложенных списков (+скорость, -ошибки округления и переполнения)\n",
    "Было:\n",
    "w = [[random.uniform(-1, 1) for _ in range(layers[i])] for _ in range(layers[i + 1])]\n",
    "Стало:\n",
    "self.weights = [np.random.uniform(-1, 1, (layers[i], layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "2. Векторизованный forward и backkward (матричное умножение в NumPy в 10-100 раз быстрее циклов, обработка батчами, а не поэлементно)\n",
    "\n",
    "3. Обработка батчей и нормализация градиентов\n",
    "\n",
    "dw = self.a[i].T @ deltas[i] / m\n",
    "db = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "Это предотвращает взрыв градиентов и делает обучение более плавным\n",
    "\n",
    "4. Улучшенная ф-ция потерь\n",
    "Было:\n",
    "loss = sum([(outputs[i][j] - y_batch[i][j])**2 for j in range(len(outputs[i]))])\n",
    "Стало:\n",
    "loss = np.mean((output - y_batch)**2)\n",
    "\n",
    "np.mean автоматически обрабатывает весь батч\n",
    "устранены ошибки округления и nan, связанные с переполнением\n",
    "стабильная и корректная работа MSE\n",
    "\n",
    "5. Увеличена learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a88062-cf8a-4c6c-ad63-b00f3214917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76336de-b46e-4bbb-8eee-3826e85266b6",
   "metadata": {},
   "source": [
    "Необходимо реализовать класс Нейронная сеть, который позволяет инициализировать архитектуру полносвязной сети с возможностью передачи кол-ва нейронов, слоёв, функций активации, а также предполагает методы для обучения и валидации.\n",
    "\n",
    "- инициализация архитектуры сети\n",
    "- прямой проход\n",
    "- обратный\n",
    "- обучение модели\n",
    "- предсказание\n",
    "- валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95359022-fb5a-4b3e-9c09-8986acb23fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation_funcs, lr=0.01):\n",
    "        self.layers = layers\n",
    "        self.activation_funcs = activation_funcs\n",
    "        self.lr = lr\n",
    "        self.weights = [np.random.uniform(-1, 1, (layers[i], layers[i+1])) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def _activation(self, x, func):\n",
    "        if func == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif func == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif func == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif func == 'linear':\n",
    "            return x\n",
    "\n",
    "    def _activation_derivative(self, x, func):\n",
    "        if func == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif func == 'sigmoid':\n",
    "            sig = self._activation(x, 'sigmoid')\n",
    "            return sig * (1 - sig)\n",
    "        elif func == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif func == 'linear':\n",
    "            return np.ones_like(x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = []\n",
    "        self.a = [X]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            z = self.a[-1] @ self.weights[i] + self.biases[i]\n",
    "            a = self._activation(z, self.activation_funcs[i])\n",
    "            self.z.append(z)\n",
    "            self.a.append(a)\n",
    "\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        output = self.a[-1]\n",
    "        y = y.reshape(output.shape)\n",
    "        deltas = [(output - y) * self._activation_derivative(self.z[-1], self.activation_funcs[-1])]\n",
    "\n",
    "        for i in reversed(range(len(self.weights) - 1)):\n",
    "            delta = deltas[0] @ self.weights[i+1].T * self._activation_derivative(self.z[i], self.activation_funcs[i])\n",
    "            deltas.insert(0, delta)\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            dw = self.a[i].T @ deltas[i] / m\n",
    "            db = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "            self.weights[i] -= self.lr * dw\n",
    "            self.biases[i] -= self.lr * db\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs=100, batch_size=32):\n",
    "        n_samples = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            total_loss = 0\n",
    "\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "                X_batch = X_train[batch_idx]\n",
    "                y_batch = Y_train[batch_idx]\n",
    "\n",
    "                output = self.forward(X_batch)\n",
    "                loss = np.mean((output - y_batch)**2)\n",
    "                total_loss += loss * len(X_batch)\n",
    "\n",
    "                self.backward(X_batch, y_batch)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                avg_loss = total_loss / n_samples\n",
    "                print(f\"Epoch {epoch}, MSE: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def calculate_mae(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(np.abs(pred - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae627cf-bb14-4958-8e84-483cc0922801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и подготовка данных\n",
    "df = pd.read_csv(\"./additional/ParisHousing.csv\")\n",
    "X = df.drop('price', axis=1).values\n",
    "Y = df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7b5bda-6f77-4d77-93eb-909fd465f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "Y_log = np.log1p(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aff5de2-d499-41c6-8c29-25ebe9badd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(X_scaled)))\n",
    "random.shuffle(indices)\n",
    "split = int(0.8 * len(X_scaled))\n",
    "train_idx = indices[:split]\n",
    "test_idx = indices[split:]\n",
    "\n",
    "X_train = X_scaled[train_idx]\n",
    "Y_train = Y_log[train_idx].reshape(-1, 1)\n",
    "X_test = X_scaled[test_idx]\n",
    "Y_test = Y_log[test_idx].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed84b0eb-b965-43d8-b53f-07e336221d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, MSE: 76.4319\n",
      "Epoch 5, MSE: 2.2469\n",
      "Epoch 10, MSE: 1.5728\n",
      "Epoch 15, MSE: 1.0662\n",
      "Epoch 20, MSE: 0.6639\n",
      "Epoch 25, MSE: 0.5878\n",
      "Epoch 30, MSE: 0.5082\n",
      "Epoch 35, MSE: 0.4555\n",
      "Epoch 40, MSE: 0.3931\n",
      "Epoch 45, MSE: 0.3536\n",
      "Epoch 50, MSE: 0.5344\n",
      "Epoch 55, MSE: 0.3212\n",
      "Epoch 60, MSE: 0.3022\n",
      "Epoch 65, MSE: 0.2416\n",
      "Epoch 70, MSE: 0.2451\n",
      "Epoch 75, MSE: 0.2188\n",
      "Epoch 80, MSE: 0.1841\n",
      "Epoch 85, MSE: 0.1863\n",
      "Epoch 90, MSE: 0.1524\n",
      "Epoch 95, MSE: 0.1569\n",
      "Epoch 100, MSE: 0.1347\n",
      "Epoch 105, MSE: 0.1254\n",
      "Epoch 110, MSE: 0.1179\n",
      "Epoch 115, MSE: 0.1203\n",
      "Epoch 120, MSE: 0.0999\n",
      "Epoch 125, MSE: 0.0932\n",
      "Epoch 130, MSE: 0.0871\n",
      "Epoch 135, MSE: 0.0831\n",
      "Epoch 140, MSE: 0.0765\n",
      "Epoch 145, MSE: 0.0706\n",
      "MAE: 0.17\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[X_train.shape[1], 64, 32, 1],\n",
    "    activation_funcs=['relu', 'relu', 'linear'],\n",
    "    lr=0.01\n",
    ")\n",
    "model.train(X_train, Y_train, epochs=150, batch_size=100)\n",
    "mae = model.calculate_mae(X_test, Y_test)\n",
    "print(f\"MAE: {mae:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
