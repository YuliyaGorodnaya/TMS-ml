{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Квантизация.\n",
        "\n",
        "**Hugging Face Optimum** - набор адаптеров к разным бэкендам.\n",
        "\n",
        "https://github.com/huggingface/optimum\n",
        "\n",
        "Плюсы: простой и понятный API, мульти-бэкенд, хорош для трансформеров.\n",
        "\n",
        "- **bitsandbytes**\n",
        "\n",
        "лёгкая квантизация весов INT8/INT4\n",
        "\n",
        "максимально простая интеграция\n",
        "\n",
        "работа с PyTorch\n",
        "\n",
        "но не даёт PTQ, только weight-only (квантованию подвергаются только веса, а активации сохраняют высокую точность)\n",
        "\n",
        "- **AutoGPTQ**\n",
        "\n",
        "качественная PTQ для LLM в INT4 с мин. потерей качества\n",
        "\n",
        "стандарт для 4х битных LLM\n",
        "\n",
        "GPTQ-квантизация (веса модели переводятся после завершения обучения)\n",
        "\n",
        "из минусов: повторно квантованную модель не обучишь\n",
        "\n",
        "- **AWQ**\n",
        "\n",
        "квантизация с учётом активаций\n",
        "\n",
        "качество выше, чем у GPTQ\n",
        "\n",
        "анализ активаций\n",
        "\n",
        "определение важных весов\n",
        "\n",
        "квантование неважных весов в INT4\n",
        "\n",
        "быстрый inference на GPU\n",
        "\n",
        "- **OpenVINO**\n",
        "\n",
        "оптимизация и INT8 для Intel CPU\n",
        "\n",
        "берём, когда у нас сервер с Intel, ноутбук или edge-устройство\n",
        "\n",
        "INT8 PTQ\n",
        "\n",
        "уменьшение энергопотребления\n",
        "\n",
        "ускорение на Xeon, Core, iGPU\n",
        "\n",
        "- **TensorRT-LLM**\n",
        "\n",
        "максимальная скорость на NVIDIA GPU\n",
        "\n",
        "FP8, INT8, INT4\n",
        "\n",
        "супербыстрый инференс\n",
        "\n",
        "поддержка GPTQ/AWQ\n",
        "\n",
        "для продакшена\n",
        "\n",
        "- **ONNX Runtime Quantization**\n",
        "\n",
        "INT8 PTQ\n",
        "\n",
        "квантование любой модели, которую можно экспортировать в ONNX (не только LLM, но и CNN, трансформеры, энкодеры)\n",
        "\n",
        "ускорение на CPU, GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "KCftWcGuOHXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Библиотека | Основная функция |\n",
        "|-----------|------------------|\n",
        "| bitsandbytes | Простая INT8/INT4 квантизация весов |\n",
        "| AutoGPTQ | Качественная PTQ INT4 для LLM |\n",
        "| AWQ | INT4 PTQ с учётом активаций (максимальное качество) |\n",
        "| OpenVINO | INT8 + оптимизация для Intel CPU |\n",
        "| TensorRT-LLM | Максимальная скорость на NVIDIA GPU |\n",
        "| ONNX Runtime | Универсальная INT8 PTQ для любых моделей |\n",
        "| Optimum | Единый API ко всем backend’ам |"
      ],
      "metadata": {
        "id": "wFZKrDbvXwGB"
      }
    }
  ]
}