{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6a88062-cf8a-4c6c-ad63-b00f3214917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76336de-b46e-4bbb-8eee-3826e85266b6",
   "metadata": {},
   "source": [
    "Необходимо реализовать класс Нейронная сеть, который позволяет инициализировать архитектуру полносвязной сети с возможностью передачи кол-ва нейронов, слоёв, функций активации, а также предполагает методы для обучения и валидации.\n",
    "\n",
    "- инициализация архитектуры сети\n",
    "- прямой проход\n",
    "- обратный\n",
    "- обучение модели\n",
    "- предсказание\n",
    "- валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95359022-fb5a-4b3e-9c09-8986acb23fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation_funcs, lr = 0.001):\n",
    "        self.layers = layers\n",
    "        self.activation_funcs = activation_funcs\n",
    "        self.lr = lr\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # for i in range(len(layers) - 1):\n",
    "        #     w = [[random.uniform(-1, 1) for _ in range(layers[i])] for _ in range(layers[i + 1])]\n",
    "        #     b = [0.0 for _ in range(layers[i + 1])]\n",
    "        #     self.weights.append(w)\n",
    "        #     self.biases.append(b)\n",
    "        for i in range(len(layers) - 1):\n",
    "            limit = math.sqrt(6 / (layers[i] + layers[i + 1]))\n",
    "            w = [[random.uniform(-limit, limit) for _ in range(layers[i])] for _ in range(layers[i + 1])]\n",
    "            b = [0.0 for _ in range(layers[i + 1])]\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def _activation(self, x, activation_func):\n",
    "        if activation_func == 'relu':\n",
    "            return max(0, x)\n",
    "        elif activation_func == 'sigmoid':\n",
    "            return 1 / (1 + math.exp(-x))\n",
    "        elif activation_func == 'tanh':\n",
    "            return math.tanh(x)\n",
    "        elif activation_func == 'linear':\n",
    "            return x\n",
    "\n",
    "    def _activation_derivative(self, x, activation_func):\n",
    "        if activation_func == 'relu':\n",
    "            return 1.0 if x > 0 else 0.0\n",
    "        elif activation_func == 'sigmoid':\n",
    "            x = max(min(x, 100), -100)  # ограничиваем диапазон\n",
    "            return 1 / (1 + math.exp(-x))\n",
    "        # elif activation_func == 'sigmoid':\n",
    "        #     sig = self._activation(x, 'sigmoid')\n",
    "        #     return sig * (1 - sig)\n",
    "        elif activation_func == 'tanh':\n",
    "            return 1 - math.tanh(x)**2\n",
    "        elif activation_func == 'linear':\n",
    "            return 1.0\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.z = []\n",
    "        self.a = [input_vector]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            z_layer = []\n",
    "            a_layer = []\n",
    "            for j in range(len(self.weights[i])):\n",
    "                z = sum([self.weights[i][j][k] * self.a[-1][k] for k in range(len(self.a[-1]))]) + self.biases[i][j]\n",
    "                a = self._activation(z, self.activation_funcs[i])\n",
    "                z_layer.append(z)\n",
    "                a_layer.append(a)\n",
    "            self.z.append(z_layer)\n",
    "            self.a.append(a_layer)\n",
    "\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, input_vector, target):\n",
    "        output = self.a[-1] # предсказанное значение\n",
    "        target = [target] if isinstance(target, (int, float)) else target # фактическое значение, в кот-ое д/б попасть\n",
    "        deltas = []\n",
    "        \n",
    "        # дельта (ошибка) выходного слоя\n",
    "        delta = [(output[i] - target[i]) * self._activation_derivative(self.z[-1][i], self.activation_funcs[-1])\n",
    "                 for i in range(len(output))]\n",
    "        deltas.append(delta)\n",
    "\n",
    "        # дельты (ошибки) скрытых слоёв\n",
    "        for i in reversed(range(len(self.weights) - 1)):\n",
    "            delta = []\n",
    "            for j in range(len(self.weights[i])):\n",
    "                # смотрим, как каждый нейрон повлиял на ошибку следующего слоя\n",
    "                error = sum([deltas[0][k] * self.weights[i + 1][k][j] for k in range(len(deltas[0]))]) \n",
    "                # домножаем на на производную активации, чтобы узнать дельту нейрона j\n",
    "                delta_j = error * self._activation_derivative(self.z[i][j], self.activation_funcs[i])\n",
    "                delta.append(delta_j)\n",
    "            deltas.insert(0, delta) # добавляем дельту в начало списка, чтобы сохранить порядок слоёв\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            for j in range(len(self.weights[i])):\n",
    "                for k in range(len(self.weights[i][j])):\n",
    "                    self.weights[i][j][k] -= self.lr * deltas[i][j] * self.a[i][k] # градиентый спуск\n",
    "                self.biases[i][j] -= self.lr * deltas[i][j]\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs=1000, batch_size=32):\n",
    "        n_samples = len(X_train)\n",
    "        for epoch in range(epochs):\n",
    "            print (f\"Epoch {epoch} started\")\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            total_loss = 0\n",
    "\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch_idx = indices[start_idx:end_idx]\n",
    "                X_batch = X_train[batch_idx]\n",
    "                y_batch = Y_train[batch_idx]\n",
    "\n",
    "                outputs = [self.forward(x.tolist()) for x in X_batch]\n",
    "\n",
    "                batch_loss = 0\n",
    "                for i in range(len(X_batch)):\n",
    "                    loss = sum([\n",
    "                        (outputs[i][j] - y_batch[i][j])**2\n",
    "                        if np.isfinite(outputs[i][j]) and np.isfinite(y_batch[i][j])\n",
    "                        else 0.0\n",
    "                        for j in range(len(outputs[i]))\n",
    "                    ])\n",
    "                    \n",
    "                    # loss = sum([(outputs[i][j] - y_batch[i][j])**2 for j in range(len(outputs[i]))])\n",
    "                    batch_loss += loss\n",
    "                    self.backward(X_batch[i].tolist(), y_batch[i].tolist())\n",
    "\n",
    "                total_loss += batch_loss\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                avg_loss = total_loss / n_samples\n",
    "                print(f\"Epoch {epoch}, MSE: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self.forward(x.tolist()) for x in X]\n",
    "\n",
    "    def calculate_mae(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        total_error = 0\n",
    "        for i in range(len(y)):\n",
    "            target = [y[i]] if isinstance(y[i], (int, float)) else y[i]\n",
    "            error = sum([abs(predictions[i][j] - target[j]) for j in range(len(predictions[i]))])\n",
    "            total_error += error\n",
    "        return total_error / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae627cf-bb14-4958-8e84-483cc0922801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и подготовка данных\n",
    "df = pd.read_csv(\"./additional/ParisHousing.csv\")\n",
    "X = df.drop('price', axis=1).values\n",
    "Y = df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7b5bda-6f77-4d77-93eb-909fd465f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "Y_log = np.log1p(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aff5de2-d499-41c6-8c29-25ebe9badd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(X_scaled)))\n",
    "random.shuffle(indices)\n",
    "split = int(0.8 * len(X_scaled))\n",
    "train_idx = indices[:split]\n",
    "test_idx = indices[split:]\n",
    "\n",
    "X_train = X_scaled[train_idx]\n",
    "Y_train = Y_log[train_idx].reshape(-1, 1)\n",
    "X_test = X_scaled[test_idx]\n",
    "Y_test = Y_log[test_idx].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed84b0eb-b965-43d8-b53f-07e336221d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация и обучение модели\n",
    "model = NeuralNetwork(\n",
    "    layers=[X_train.shape[1], 64, 32, 1],\n",
    "    activation_funcs=['relu', 'relu', 'linear'],\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69f276c9-7c1f-43bc-95cf-7bd2c51b4f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n",
      "Epoch 0, MSE: 82589168507495832008720493638633845211505927531858423000562986088143660526649852409794360571928999716802790966318589687719068168509799132593830804308553366152043644676025858094678560213458330883601647507624802999937268780293276335461794456685926668137259696066938900385916979773440.0000\n",
      "Epoch 1 started\n",
      "Epoch 2 started\n",
      "Epoch 3 started\n",
      "Epoch 4 started\n",
      "Epoch 5 started\n",
      "Epoch 5, MSE: 0.0000\n",
      "Epoch 6 started\n",
      "Epoch 7 started\n",
      "Epoch 8 started\n",
      "Epoch 9 started\n",
      "Epoch 10 started\n",
      "Epoch 10, MSE: 0.0000\n",
      "Epoch 11 started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 103\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X_train, Y_train, epochs, batch_size)\u001b[0m\n\u001b[0;32m    100\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_train[batch_idx]\n\u001b[0;32m    101\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m Y_train[batch_idx]\n\u001b[1;32m--> 103\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x\u001b[38;5;241m.\u001b[39mtolist()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_batch]\n\u001b[0;32m    105\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_batch)):\n",
      "Cell \u001b[1;32mIn[15], line 53\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, input_vector)\u001b[0m\n\u001b[0;32m     51\u001b[0m a_layer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i])):\n\u001b[1;32m---> 53\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i][j][k] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i][j]\n\u001b[0;32m     54\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation(z, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_funcs[i])\n\u001b[0;32m     55\u001b[0m     z_layer\u001b[38;5;241m.\u001b[39mappend(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(X_train, Y_train, epochs=150, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a21de-1911-4092-9ee7-0c02f45236e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = model.calculate_mae(X_train, Y_train)\n",
    "mae_test = model.calculate_mae(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
