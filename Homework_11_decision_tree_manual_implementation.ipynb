{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1760357892862,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "Y72xmuIFdXR-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth  # максимальная глубина дерева\n",
    "        self.min_samples_split = min_samples_split  # минимальное количество образцов для разбиения\n",
    "        self.tree = None  # здесь будет храниться построенное дерево\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # cтроим дерево по обучающим данным\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # предсказываем метки (y) для новых данных\n",
    "        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        # считаем индекс Джини — меру неоднородности (насколько однородны данные)\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        # находим наилучшее разбиение по всем признакам\n",
    "        best_gini = float('inf')\n",
    "        best_index, best_threshold = None, None\n",
    "\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_index] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if len(y[left_mask]) < self.min_samples_split or len(y[right_mask]) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                gini_left = self._gini(y[left_mask])\n",
    "                gini_right = self._gini(y[right_mask])\n",
    "                weighted_gini = (len(y[left_mask]) * gini_left + len(y[right_mask]) * gini_right) / len(y)\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_index, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # рекурсивно строим дерево\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        # условия остановки\n",
    "        if (depth >= self.max_depth or num_labels == 1 or num_samples < self.min_samples_split): # достигнута максимальная глубина\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'leaf': True, 'value': leaf_value}\n",
    "\n",
    "        feature_index, threshold = self._best_split(X, y) # все объекты в листе с одинаковой меткой\n",
    "        if feature_index is None:\n",
    "            return {'leaf': True, 'value': self._most_common_label(y)}\n",
    "\n",
    "        left_mask = X[:, feature_index] <= threshold # в узле слишком мало объектов для разбиения\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature_index': feature_index,\n",
    "            'threshold': threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _predict_sample(self, sample, tree):\n",
    "        # предсказание для одного примера\n",
    "        if tree['leaf']:\n",
    "            return tree['value']\n",
    "\n",
    "        if sample[tree['feature_index']] <= tree['threshold']:\n",
    "            return self._predict_sample(sample, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(sample, tree['right'])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # наиболее частая метка в листе - наше предсказание\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2760,
     "status": "ok",
     "timestamp": 1760357902555,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "zKhyKOMBgVKp",
    "outputId": "6d983c37-81cb-4d6e-fc7e-ac688c206a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность дерева решений: 0.97\n"
     ]
    }
   ],
   "source": [
    "# тест-драйв\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Генерируем простой датасет\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=2, random_state=42)\n",
    "\n",
    "# 2. Делим на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Обучаем наше дерево\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# 4. Предсказываем на тесте\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# 5. Оцениваем точность\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Точность дерева решений: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1760358561075,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "HSGQPEnog9LZ"
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators  # количество деревьев\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features  # сколько признаков использовать в каждом дереве\n",
    "        self.trees = []  # список деревьев\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # обучаем лес: каждое дерево на случайной подвыборке\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        self.max_features = self.max_features or int(np.sqrt(n_features))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # случайная выборка\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            # случайный набор признаков\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "            X_sample_subset = X_sample[:, feature_indices]\n",
    "\n",
    "            # то есть у нас каждое дерево обучается на ограниченном кол-ве признаков и строк исходной матрицы\n",
    "\n",
    "            # обучаем дерево\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_sample_subset, y_sample)\n",
    "\n",
    "            # сохраняем дерево и признаки, на которых оно обучалось\n",
    "            self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # предсказание: голосование деревьев\n",
    "        tree_preds = []\n",
    "\n",
    "        for tree, feature_indices in self.trees:\n",
    "            X_subset = X[:, feature_indices]\n",
    "            preds = tree.predict(X_subset)\n",
    "            tree_preds.append(preds)\n",
    "\n",
    "        # транспонируем, чтобы получить матрицу, где каждая строка — предсказания всех деревьев для одного объекта\n",
    "        tree_preds = np.array(tree_preds).T\n",
    "\n",
    "        # голосование: выбираем наиболее частый класс - это и есть наш ответ\n",
    "        final_preds = [Counter(row).most_common(1)[0][0] for row in tree_preds]\n",
    "        return np.array(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1760358701683,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "MExxLV4gi0z5",
    "outputId": "fe3cd37d-0f7a-4b03-810f-f5261ac83d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность RandomForest: 0.97\n"
     ]
    }
   ],
   "source": [
    "# обучение нашего леса\n",
    "forest = RandomForest(n_estimators=50, max_depth=5, max_features=2)\n",
    "# forest = RandomForest(n_estimators=10, max_depth=3) - никудышные параметры (accuracy 0.73)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# предсказание\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# оценка точности\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Точность RandomForest: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1760359389069,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "P4-be_XZmA3y"
   },
   "outputs": [],
   "source": [
    "# модель рассчитывает уверенность в классе, а следующая учится на её ошибке, типа предыдущ. был на 0.8 уверена в классе А, а это класс Б. Тогда следующее дерево понизит уверенность\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # преобразуем метки в {-1, 1}\n",
    "        y_transformed = np.where(y == 1, 1, -1)\n",
    "        self.init_pred = np.zeros(len(y))  # начальное предсказание — ноль\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # градиент логистической функции потерь\n",
    "            residuals = y_transformed / (1 + np.exp(y_transformed * self.init_pred))\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, residuals)\n",
    "            update = tree.predict(X)\n",
    "\n",
    "            # обновляем предсказания\n",
    "            self.init_pred += self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "        proba = self._sigmoid(pred)\n",
    "        return np.vstack([1 - proba, proba]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1760359526363,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "AW5cdyg4mkQp",
    "outputId": "dd68ed6b-0541-429f-9332-82d3761dc898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность Gradient Boosting: 0.97\n"
     ]
    }
   ],
   "source": [
    "# обучаем градиентный бустинг\n",
    "gb = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_depth=2)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# предсказания\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# оценка точности\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Точность Gradient Boosting: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1760359964564,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "9HxNvigHoA6S"
   },
   "outputs": [],
   "source": [
    "# теперь всё то же самое для задачи регрессии\n",
    "# вместо джинни - MSE; возвращаем среднее значение целевой переменной в листе вместо класса\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
    "\n",
    "    def _mse(self, y):\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_mse = float('inf')\n",
    "        best_index, best_threshold = None, None\n",
    "\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_index] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if len(y[left_mask]) < self.min_samples_split or len(y[right_mask]) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                mse_left = self._mse(y[left_mask])\n",
    "                mse_right = self._mse(y[right_mask])\n",
    "                weighted_mse = (len(y[left_mask]) * mse_left + len(y[right_mask]) * mse_right) / len(y)\n",
    "\n",
    "                if weighted_mse < best_mse:\n",
    "                    best_mse = weighted_mse\n",
    "                    best_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_index, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split:\n",
    "            return {'leaf': True, 'value': np.mean(y)}\n",
    "\n",
    "        feature_index, threshold = self._best_split(X, y)\n",
    "        if feature_index is None:\n",
    "            return {'leaf': True, 'value': np.mean(y)}\n",
    "\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature_index': feature_index,\n",
    "            'threshold': threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _predict_sample(self, sample, tree):\n",
    "        if tree['leaf']:\n",
    "            return tree['value']\n",
    "        if sample[tree['feature_index']] <= tree['threshold']:\n",
    "            return self._predict_sample(sample, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(sample, tree['right'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760360034625,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "3TeSXaZLoYhT"
   },
   "outputs": [],
   "source": [
    "# усредняем результаты голосования\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "        self.max_features = self.max_features or int(np.sqrt(n_features))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "            X_subset = X_sample[:, feature_indices]\n",
    "\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_subset, y_sample)\n",
    "            self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tree, feature_indices in self.trees:\n",
    "            X_subset = X[:, feature_indices]\n",
    "            preds = tree.predict(X_subset)\n",
    "            predictions.append(preds)\n",
    "        return np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1760360125531,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "UWFhDUahorkC"
   },
   "outputs": [],
   "source": [
    "# каждая последующая модель пытается предсказать то, что не хватило предыдущей\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.init_pred = np.mean(y)\n",
    "        pred = np.full(len(y), self.init_pred)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, residuals)\n",
    "            update = tree.predict(X)\n",
    "\n",
    "            pred += self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.full(X.shape[0], self.init_pred)\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5588,
     "status": "ok",
     "timestamp": 1760360414084,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "8lH3qZQjp9qr",
    "outputId": "f6c2d9f9-5441-4ab0-e795-9541ace6421c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree R²: 0.72\n",
      "Random Forest R²: 0.53\n",
      "Gradient Boosting R²: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Генерация данных\n",
    "X, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# 2. Делим на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Обучение моделей\n",
    "dt = DecisionTreeRegressor(max_depth=5)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=50, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "# 4. Оценка качества\n",
    "print(f\"Decision Tree R²: {r2_score(y_test, y_pred_dt):.2f}\")\n",
    "print(f\"Random Forest R²: {r2_score(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"Gradient Boosting R²: {r2_score(y_test, y_pred_gb):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11388,
     "status": "ok",
     "timestamp": 1760360510848,
     "user": {
      "displayName": "Юлия Гор.",
      "userId": "08838257762277085118"
     },
     "user_tz": -180
    },
    "id": "Zb2S_yfzqXyS",
    "outputId": "7475ecda-be37-486f-e745-395e748a7d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R² (обновлённый): 0.85\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    max_features=X.shape[1]\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Random Forest R² (обновлённый): {r2_score(y_test, y_pred_rf):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMzVQ8oRKJB19OrSQmdkKuf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
